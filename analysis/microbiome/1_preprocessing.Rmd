---
title: "1_preprocessing"
author: "GQ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r warning=FALSE}
library(Biostrings)
library(ShortRead)
library(dada2)
```

We used the data/raw_fastq/ directory to store our fastq files. Make sure you use the appropriate path

```{r}
fnFs <- sort(list.files("data/raw_fastq/", pattern="_R1_001.fastq", 
                        full.names = TRUE))
fnRs <- sort(list.files("data/raw_fastq/", pattern="_R2_001.fastq", 
                        full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample_names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

# Inspect read quality profiles

We start by visualizing the quality profiles of the forward reads:

```{r}
plotQualityProfile(fnFs[1:2])

plotQualityProfile(fnRs[1:2])
```

# Primer presence

```{r}
fwd_primer <- "GTGYCAGCMGCCGCGGTAA"
rev_primer <- "GGACTACNVGGGTWTCTAAT"
```

To ensure we have the right primers, and the correct orientation of the primers on the reads, we will verify the presence and orientation of these primers in the data.

```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

fwd_orients <- allOrients(fwd_primer)
rev_orients <- allOrients(rev_primer)

fwd_orients
rev_orients
```

## Filter

```{r}
fnFs_filtN <- file.path("output", "N-filtered", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory

fnRs_filtN <- file.path("output", "N-filtered", basename(fnRs))

filterAndTrim(fnFs, fnFs_filtN, fnRs, fnRs_filtN, 
              maxN = 0, multithread = TRUE)
```


We are now ready to count the number of times the primers appear in the forward and reverse read, while considering all possible primer orientations. Identifying and counting the primers on one set of paired end FASTQ files is sufficient, assuming all the files were created using the same library preparation, so we’ll just process the first sample.


```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(fwd_orients, primerHits, fn = fnFs_filtN[[1]]), 
    FWD.ReverseReads = sapply(fwd_orients, primerHits, fn = fnRs_filtN[[1]]), 
    REV.ForwardReads = sapply(rev_orients, primerHits, fn = fnFs_filtN[[1]]), 
    REV.ReverseReads = sapply(rev_orients, primerHits, fn = fnRs_filtN[[1]]))
```

# Primer removal

First let's set the path to the cutadapt command adn check that it's detected by asking its version

```{r}
cutadapt <-"/usr/bin/cutadapt" #check local path of installation

system2(cutadapt, args = "--version")
```


```{r}
path_cut <- file.path("output", "cutadapt")

if(!dir.exists(path_cut)) dir.create(path_cut)

fnFs_cut <- file.path(path_cut, basename(fnFs[1:10]))
fnRs_cut <- file.path(path_cut, basename(fnRs[1:10]))

FWD_RC <- dada2:::rc(fwd_primer)
REV_RC <- dada2:::rc(rev_primer)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1_flags <- paste("-g", fwd_primer, "-a", REV_RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2_flags <- paste("-G", rev_primer, "-A", FWD_RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1_flags, R2_flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs_cut[i], "-p", fnRs_cut[i], # output files
                             fnFs_filtN[i], fnRs_filtN[i])) # input files
}
```

## Verification

```{r}
rbind(FWD.ForwardReads = sapply(fwd_orients, primerHits, fn = fnFs_cut[[1]]), 
    FWD.ReverseReads = sapply(fwd_orients, primerHits, fn = fnRs_cut[[1]]), 
    REV.ForwardReads = sapply(rev_orients, primerHits, fn = fnFs_cut[[1]]), 
    REV.ReverseReads = sapply(rev_orients, primerHits, fn = fnRs_cut[[1]]))
```

The primer-free sequence files are now ready to be analyzed through the DADA2 pipeline. Similar to the earlier steps of reading in FASTQ files, we read in the names of the cutadapt-ed FASTQ files and applying some string manipulation to get the matched lists of forward and reverse fastq files.


```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path = "output/cutadapt", 
                         pattern = "_R1_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path = "output/cutadapt",
                         pattern = "_R2_001.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get_sample_name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample_names <- unname(sapply(cutFs, get_sample_name))
head(sample_names)
```


## Inspect read quality profiles

We start by visualizing *again* the quality profiles of the forward reads:

```{r}
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])

```

# Filter and trim

```{r}
filtFs <- file.path("output", "filtered", basename(cutFs))
filtRs <- file.path("output", "filtered", basename(cutRs))
```


```{r}
t0 <- Sys.time()

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
                     truncLen = c(200,200),
                     #trimLeft = 20,
                     #truncLen = c(290, 270), #Grazia's values
                     maxN = 0, 
                     maxEE = c(2, 2), 
                     truncQ = 2, 
                     rm.phix = TRUE,
                     compress = TRUE, 
                     multithread = TRUE) # On Windows set multithread=FALSE

head(out)

tf <- Sys.time() - t0
tf
```

# Learn the Error Rates

The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r}
t0 <- Sys.time()

errF <- learnErrors(filtFs, multithread = TRUE)

errR <- learnErrors(filtRs, multithread = TRUE)

plotErrors(errF, nominalQ = TRUE)

tf <- Sys.time() - t0
tf
```

# Sample Inference
We are now ready to apply the core sample inference algorithm to the filtered and trimmed sequence data.

```{r}
t0 <- Sys.time()

dadaFs <- dada(filtFs, err = errF, multithread = TRUE)

dadaRs <- dada(filtRs, err = errR, multithread = TRUE)

tf <- Sys.time() - t0
tf
```

Inspecting the returned dada-class object:

```{r}
dadaFs[[1]]
```

# Merge paired reads

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

The mergers object is a list of data.frames from each sample. Each data.frame contains the merged $sequence, its $abundance, and the indices of the $forward and $reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.

# Construct sequence table
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r}
seqtab <- makeSequenceTable(mergers)

dim(seqtab)
```

Inspect distribution of sequence lengths

```{r}
table(nchar(getSequences(seqtab)))
```

# Remove chimeras
The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", 
                                    multithread = TRUE, verbose = TRUE)

dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)
```

# Track reads through the pipeline

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r}
getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), 
               sapply(mergers, getN), rowSums(seqtab.nochim))

# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF",
                     "denoisedR", "merged", "nonchim")

rownames(track) <- sample_names

head(track)
```

# Assign taxonomy

It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The assignTaxonomy function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence.

We maintain formatted training fastas for the RDP training set, GreenGenes clustered at 97% identity, and the Silva reference database, and additional trainings fastas suitable for protists and certain specific environments have been contributed. For fungal taxonomy, the General Fasta release files from the UNITE ITS database can be used as is. To follow along, download the silva_nr_v132_train_set.fa.gz file, and place it in the directory with the fastq files.

```{r}
t0 <- Sys.time()

taxa <- assignTaxonomy(seqtab.nochim, "reference_db/silva_nr99_v138.1_train_set.fa.gz", #make sure you're using the correct path
                       multithread = TRUE)
tf <- Sys.time() - t0
tf


t0 <- Sys.time()
taxa <- addSpecies(taxa, "reference_db/silva_species_assignment_v138.1.fa.gz")

tf <- Sys.time() - t0
tf
```

Let’s inspect the taxonomic assignments:

```{r}
taxa_print <- taxa # Removing sequence rownames for display only

rownames(taxa_print) <- NULL

head(taxa_print)

write.csv(taxa_print, "output/asv_raw.csv",
          row.names = FALSE)
```



